# -*- coding: utf-8 -*-
"""Business Data Analytics Model Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tGeyYpp23wkjcAqPZT8JUiJKSXheUpqR
"""

!pip install datatable

import pandas as pd
import numpy as np
import datatable as dt

"""### Import Data from Google Drive (personal account)"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

Data = dt.fread("drive/My Drive/Data_For_Model.csv").to_pandas()

# print(Data.shape) # (2402909, 36)
# print(Data.dropna().shape) # (1669220, 36)

# Data.dropna()["LuckyOrNot"].value_counts()
# UnLucky    1588699
# Lucky        80521

"""### Check Histogram Before and After Dropna"""

# import matplotlib.pyplot as plt
# plotsize = plt.figure(figsize = (15, 20))
# ax = plotsize.gca()  
# # Data before dropping NaN
# Data.hist(ax = ax)

# plotsize = plt.figure(figsize = (15, 20))
# ax = plotsize.gca()  
# # print("Data after dropping NaN")
# Data.dropna().hist(ax = ax)

"""> **_Except Age_of_Driver has a little change, but I think the difference is not serious since the age that lower than 20 is not even able to drive a car_**

### Drop Missing Value
"""

# Since the data is adequate, we decided to remove all the row that contains NaN
Data_dropna = Data.dropna()

"""### Convert Dummy Variables"""

# Get Dummy Variables
DummyCols = Data.dtypes.index[np.where((Data.dtypes == "object") & (Data.dtypes.index != "LuckyOrNot"))]
Population = pd.get_dummies(data = Data_dropna, columns = DummyCols,dtype = np.int16)
Population.head()
del DummyCols

"""---"""

import sklearn
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix
import seaborn as sns

"""### Model Sampling"""

# Since the Response proportion has a huge gap between two labels
# We use Under-sampling (prototype selection)
def Undersampling(data, label) :
  Train = data
  lev1 = Train[label].value_counts().index[0]
  lev2 = Train[label].value_counts().index[1]
  val1 = Train[label].value_counts().values[0]
  val2 = Train[label].value_counts().values[1]

  if val1 > val2 :
    largerSample = lev1
    smallerSample = lev2
    del lev1, lev2
  else :
    largerSample = lev2
    smallerSample = lev1
    del lev1, lev2
  
  LargeSampleData = Train[Train[label] == largerSample]
  SmallSampleData = Train[Train[label] == smallerSample]

  ExtractNrow = SmallSampleData.shape[0]
  subsampleIndex = np.random.randint(0, LargeSampleData.shape[0]-1,ExtractNrow)

  Chosen_LargeSample = LargeSampleData.iloc[subsampleIndex,:]
  Under_Sample_SubSample = pd.concat([SmallSampleData, Chosen_LargeSample], axis = 0)

  return Under_Sample_SubSample
# Function End

UnderSample_population = Undersampling(data = Population, label = "LuckyOrNot")

# Create Train Test Data with constant response term proportion
trainX, testX, trainY, testY = train_test_split(UnderSample_population.iloc[:, 1:], 
                                                UnderSample_population.iloc[:, 0], 
                                                shuffle = True, 
                                                stratify = UnderSample_population.iloc[:, 0],
                                                train_size = 0.7)
# Sampling End

Lencoder = LabelEncoder().fit(["Lucky", "UnLucky"])

"""### Random Forest in XGB"""

import xgboost as xgb

Lencoder = LabelEncoder().fit(["Lucky", "UnLucky"])
LabeledtrainY = Lencoder.transform(trainY)
# train_DMatrix = xgb.DMatrix(trainX, LabeledtrainY)

### Train RF with xgboost package
params = {
  'colsample_bynode': 0.5,
  'learning_rate': 0.01,
  'max_depth': 50,
  'num_parallel_tree': 250,
  'objective': 'binary:logistic',
  'subsample': 0.7,
  'tree_method': 'gpu_hist'
}

XGB_RF = xgb.train(params, train_DMatrix, num_boost_round = 1)

Lev1_prob = XGB_RF.predict(xgb.DMatrix(testX))
Res = Lencoder.inverse_transform(np.where(Lev1_prob >=0.5, 1, 0))

# Validate Model
def Scores(true_y, pred_y_prob) :
  Lencoder = LabelEncoder().fit(true_y.value_counts().index)

  trueY_Label = true_y
  trueY_Encode = Lencoder.transform(true_y)
  pred_y_prob = pred_y_prob
  pred_y_Encode = np.where(pred_y_prob >= 0.5, 1, 0)
  pred_y_Label = Lencoder.inverse_transform(pred_y_Encode)

  print("Accuracy : ", metrics.accuracy_score(trueY_Encode, pred_y_Encode))
  print("Recall : ", metrics.recall_score(trueY_Encode, pred_y_Encode))
  print("Precision : ", metrics.precision_score(trueY_Encode, pred_y_Encode))
  print("f1-score : ", metrics.f1_score(trueY_Encode, pred_y_Encode))
  print("Auc :", metrics.roc_auc_score(trueY_Encode, pred_y_prob))

# Scores(testY, Lev1_prob)

import matplotlib.pyplot as plt

# ROC Curve
fpr, tpr, thresholds = metrics.roc_curve(Lencoder.transform(testY), Lev1_prob)
plt.plot(fpr, tpr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Random Forest in xgb ROC Curve")
plt.show()

"""### CatBoost Classifier (No Need Dummy)"""

!pip install CatBoost

from catboost import CatBoostClassifier, Pool, FeaturesData

# UnderSampling for Catboost
Data_dropna_underS = Undersampling(Data_dropna, "LuckyOrNot")
# End

# Create Train Test Data with constant response term proportion
catrainX, catestX, catrainY, catestY = train_test_split(Data_dropna_underS.loc[:,Data_dropna.columns != 'LuckyOrNot'], 
                                                        Data_dropna_underS.iloc[:, 27], 
                                                        shuffle = True, 
                                                        stratify = Data_dropna_underS.iloc[:, 27],
                                                        train_size = 0.7)
# End

# Create Pool train test data for catboost
categorical_features_indices = np.where(catrainX.dtypes != np.float)[0]
train_pool = Pool(catrainX, catrainY, cat_features=categorical_features_indices)
test_pool = Pool(catestX, cat_features=categorical_features_indices)
# End

# Fit the Model!
catboost_model = CatBoostClassifier(iterations=1000, 
                                    loss_function = "Logloss",
                                    custom_metric = "F1",
                                    learning_rate = 0.1,
 #                                   bootstrap_type = "Bayesian",
                                    depth = 5,
                                    min_data_in_leaf = 1, 
                                    task_type = "GPU",
                                    silent = True)

catboost_model.fit(train_pool)

from sklearn.model_selection import GridSearchCV
parms = {"learning_rate" : [1, 0.5, 0.1],
         "iterations" : [750, 1000, 1250],
         "depth" :[3,6,10]}
catboost_model_grid = CatBoostClassifier(loss_function = "Logloss",
                                         custom_metric = "F1",
                                         task_type = "GPU", 
                                         cat_features = categorical_features_indices,
                                         min_data_in_leaf = 1,
                                         silent = True)
Grid_Catboost = GridSearchCV(estimator = catboost_model_grid, param_grid = parms, cv = 5, n_jobs = 1)
Grid_Catboost.fit(catrainX, catrainY)

print("\n========================================================")
print(" Results from Grid Search " )
print("========================================================")    
    
print("\n The best estimator across ALL searched params:\n", Grid_Catboost.best_estimator_)
    
print("\n The best score across ALL searched params:\n", Grid_Catboost.best_score_)
    
print("\n The best parameters across ALL searched params:\n", Grid_Catboost.best_params_)
    
print("\n ========================================================")

# Predict 
cat_pred_label = catboost_model.predict(test_pool, prediction_type="Class")
cat_pred_prob = catboost_model.predict(test_pool, prediction_type="Probability")
catboost_res_DF = pd.DataFrame(cat_pred_prob, columns=["Lucky", "UnLucky"]).assign(Label = cat_pred_label)

# Accuracy
catboost_model.score(X = catestX, y = catestY)

# AUC
# AUC_array = catboost_model.eval_metrics(data = Pool(catestX, catestY,cat_features=categorical_features_indices), metrics="AUC")["AUC"]
# catboost_model_AUC = AUC_array[len(AUC_array)-1]
# print("inModel AUC: ", catboost_model_AUC)

fpr, tpr, thresholds = metrics.roc_curve(np.where(catestY == "Lucky", 1, 0), catboost_res_DF["Lucky"])
print("sklearn AUC: ", metrics.auc(fpr, tpr))

# Recall
metrics.recall_score(Lencoder.transform(catestY), Lencoder.transform(cat_pred_label))

# Precision
metrics.precision_score(Lencoder.transform(catestY), Lencoder.transform(cat_pred_label))

# f1-score
metrics.f1_score(catestY, cat_pred_label, pos_label="Lucky")

import matplotlib.pyplot as plt

VarImpShap = catboost_model.get_feature_importance(data = train_pool, type="ShapValues")
VarImp = catboost_model.get_feature_importance(data = train_pool)

CatVarImpDF = pd.DataFrame({"Feature" : catrainX.columns, "Importance" : VarImp})

plt.figure(figsize=(6,8))
plt.barh(CatVarImpDF.sort_values(by="Importance")["Feature"], CatVarImpDF.sort_values(by="Importance")["Importance"])
plt.tick_params(labelsize=12)
plt.show()

!pip install shap

import shap
shap.initjs()

explainer = shap.TreeExplainer(catboost_model)
shap_values = explainer.shap_values(catestX) 
shap.force_plot(explainer.expected_value, shap_values[0,:], catestX.iloc[0,:])

shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[0:100,:], catestX.iloc[0:100, :])

shap.summary_plot(shap_values, catrainX)

shap.summary_plot(shap_values, catrainX, plot_type="bar")

